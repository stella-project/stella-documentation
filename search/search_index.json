{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"STELLA Technical Documentation","text":"<p>Welcome to the official technical documentation for the STELLA Project. This site serves as the single source of truth for everything related to STELLA, including tutorials, demos, the STELLA microservice, STELLA app, STELLA server and REST API endpoints.</p>"},{"location":"#roles-documentation-structure","title":"Roles &amp; Documentation Structure","text":"<p>The documentation is organized around the three main roles within the STELLA ecosystem:</p>"},{"location":"#1-researcher","title":"1. Researcher","text":"<p>For researchers conducting experiments with STELLA:</p> <ul> <li>Propose and test modifications to search systems.</li> <li>Run and analyze online experiments.</li> <li>Participate in the Lilas-Lab environment.</li> </ul>"},{"location":"#2-site-owner","title":"2. Site Owner","text":"<p>For site owners hosting search systems:</p> <ul> <li>Deploy the STELLA infrastructure to their platform.</li> <li>Enable researchers to integrate experimental systems.</li> <li>Interact primarily with the REST endpoints of the STELLA app.</li> </ul>"},{"location":"#3-developer","title":"3. Developer","text":"<p>For developers working with STELLA source code:</p> <ul> <li>Explore the architecture of STELLA\u2019s core components.</li> <li>Access in-depth implementation details.</li> <li>Extend or contribute to STELLA\u2019s microservices and APIs.</li> </ul>"},{"location":"about/","title":"About STELLA","text":"<p>STELLA (InfraSTrucutrEs for Living LAbs) offers an Evaluation-as-a-Service platform for living lab experiments with ranking and recommender systems. </p> <p>With STELLA, researchers can evaluate their experimental systems based on user feedback which stands in contrast to (or complements) the Cranfield-style approaches with test collections in offline evaluations. STELLA facilitates conventional A/B tests but also more data-efficient interleaving experiments in which results lists of two ranking or recommender functions are mixed.</p> <p>A fundamental component of STELLA is the integration of experimental systems as micro-services. While previous living labs restricted the system results to the most popular top-k queries, we allow more comprehensive evaluations by integrating micro-services with entire retrieval and recommender systems. </p> <p>The Living Labs for Academic Search (LiLAS) lab at CLEF made use of the STELLA infrastructure and served as the first test-bed to evaluate the feasibility of our new infrastructure design. We welcome contributions and look for collaborations with researchers and sites alike. </p>"},{"location":"developer/common_issues/","title":"Common Errors and Troubleshooting:","text":""},{"location":"developer/common_issues/#docker-network-error","title":"Docker Network Error","text":"<p>When the docker compose stack is started this error occurs:</p> <p>Failure</p> <pre><code>network stella-shared declared as external, but could not be found\n</code></pre> <p>Solution: All container need to be in the same docker network. This network is created by the stella-server. If you run the stella-app without the stella-server or renamed the network, these changes need to be propagated to all other compose stacks.</p>"},{"location":"developer/common_issues/#cron-job-error","title":"Cron Job Error","text":"<p>The stella-app logs repeatably errors like this:</p> <p>Failure</p> <pre><code>Job \"update_server (trigger: interval[0:00:03], next run at: 2025-06-02 10:16:25 UTC)\" raised an exception\n</code></pre> <p>This is most likely related to the scheduler job that transfers the data between the stella-app and the stella-server.</p> <p>Solution: Ensure that the both databases are initialized correctly, that the required systems are available and that their IDs align. Also make sure that the addresses and ports are correctly referenced and that all container are in the same network.</p>"},{"location":"developer/common_issues/#so-results-in-the-frontend","title":"So Results in the Frontend","text":"<p>The stella-search returns no results for a query and the stella-app logs this error:</p> <p>Failure</p> <pre><code>File \"/app/app/services/ranking_service.py\", line 40, in extract_hits\n    if isinstance(hits[0], dict):\nIndexError: list index out of range\n</code></pre> <p>This can happen if the query does not match the dataset. Try a query like <code>arbeit</code>. If this still happens, the experimental systems might not be indexed correctly. Please repeat the indexing.</p>"},{"location":"developer/env-mca/","title":"STELLA-App","text":"Environment variable Meaning RANKSYS_LIST RANKSYS_PRECOM_LIST RANKSYS_BASE STELLA_SERVER_ADDRESS STELLA_SERVER_USER STELLA_SERVER_PASS STELLA_SERVER_USERNAME INTERLEAVE You can choose to interleave the results with the baseline system via <code>INTERLEAVE</code>. Per default interleaving is activated (<code>INTERLEAVE=True</code>) BULK_INDEX If all systems should start the indexing when launching the app leave <code>BULK_INDEX=True</code>. If <code>BULK_INDEX=False</code>, you can visit <code>&lt;ip-of-stella-app&gt;/</code> and index the systems individually or by calling the corresponding REST-endpoints. DELETE_SENT_SESSION If <code>DELETE_SENT_SESSION=True</code>, all sessions will removed after they have been sent to the <code>stella-server</code>. INTERVAL_DB_CHECK This environment variables controls the time interval for checking the database for finished sessions. Time is specified in seconds, i.e. <code>INTERVAL_DB_CHECK=3</code> will check every 3 seconds if there are new sessions that can be sent to the <code>stella-server</code>. SESSION_EXPIRATION This environment variables controls the time interval after the state of a sessions will automatically be set to \"finished\". Time is specified in seconds, i.e. <code>SESSION_EXPIRATION=6</code> will make a session expire after 6 seconds."},{"location":"developer/env-server/","title":"STELLA-Server","text":"Environment variable Meaning FLASK_CONFIG POSTGRES_USER POSTGRES_PW POSTGRES_DB SECRET_KEY AUTOMATOR_GH_KEY ADMIN_MAIL ADMIN_PASS GESIS_MAIL GESIS_PASS LIVIVO_MAIL LIVIVO_PASS PARTA_MAIL PARTA_PASS PARTB_MAIL PARTB_PASS POSTGRES_USER POSTGRES_PASSWORD POSTGRES_DB"},{"location":"developer/initial_setup/","title":"Setup for Local Developments","text":"<p>This guide walks you through how the STELLA infrastructure can be setup locally on one single machine for development and testing purposes.</p> <p>For local development, we recommend using the docker compose files with a <code>dev</code> suffix. They mount the local directory containing the source code and run the applications with hot reloading. By that, the applications can directly be modified while also connect to other applications in the docker network.</p>"},{"location":"developer/initial_setup/#1-clone-repositories-and-dataset","title":"1. Clone Repositories and Dataset","text":"<p>We will clone the <code>stella-app</code>, <code>stella-server</code>, and the <code>stella-search</code> repositories. All experimental containers will be automatically pulled by the stella-app compose file.</p> <pre><code>gh repo clone stella-project/stella-app\n</code></pre> <pre><code>gh repo clone stella-project/stella-server\n</code></pre> <pre><code>gh repo clone stella-project/stella-search\n</code></pre> <p>Additionally we need to add a dataset that should be searched through. In this example, we download the data from this link. However, any other dataset can be used as long as it matches the indexing pipelines in the experimental systems.</p> <p>After downloading the dataset, it needs to be extracted and placed into the directories. <pre><code>tar -xf ~/Downloads/gesis-search.tar  \n\ncp gesis-search/documents/publication.jsonl stella-search/data/index \n\ncp -r gesis-search stella-app/data/\n</code></pre></p>"},{"location":"developer/initial_setup/#2-start-stella-server","title":"2. Start Stella-Server","text":"<p>Now we can start the stella server. If you prefer to run the stella-app without the stella-server, you can skip this step.</p> <p>To start the stella-server, we navigate into the <code>stella-server</code> directory and run the compose stack: <pre><code>docker compose -f docker-compose-dev.yml\n</code></pre></p> <p>After the stella-server compose stack is running and no errors appear in the logs, its database need to be initiated. Therefore we login to the docker container and run a flask cli command that creates all tables: <pre><code>docker exec -it stella-dev-server-1 flask init-db\n</code></pre></p> <p>Now we can additionally run a command that fills the tables with the systems and users that are defines in the compose stack: <pre><code>docker exec -it stella-dev-server-1 flask seed-db\n</code></pre></p> <p>A good validation if this was successful is to check if the expected systems and users are created in the database.</p>"},{"location":"developer/initial_setup/#3-start-stella-app","title":"3. Start Stella-App","text":"<p>If the stella-app is run without the stella-server, the stella-app compose stack needs to be modified accordingly. Especially the cron service that copies data between the two databases and the docker network needs to be modified.</p> <p>Warning</p> <p>At the first startup, the cron service might log some errors because the stella-app database is not initialized yet. This is expected but should be resolved after initializing the database.</p> <p>Like before, we initialize the database of for the stella-app: <pre><code>docker exec -it stella-dev-server-1 flask init-db\n</code></pre> and also: <pre><code>docker exec -it stella-dev-server-1 flask seed-db\n</code></pre></p>"},{"location":"developer/initial_setup/#4-index-experimental-systems","title":"4. Index Experimental Systems","text":"<p>After the stella-app and the stella-server is running, we need to index the dataset if the stella-app is not configured to index the data on startup. The systems are specified in the stella-app docker compose stack and should run by now. To trigger the indexing, the stella-app user interface at localhost:8080 can be used. </p>"},{"location":"developer/initial_setup/#5-start-stella-search","title":"5. Start Stella-Search","text":"<p>Stella search mimics the search service the stella-app is deployed to. Like the other container it can be started by navigating to the stella-search directory and running <pre><code>docker compose up\n</code></pre></p> <p>The system should start and shortly after report that 110420 and 99541 documents were indexed from two directories.</p> <p>Now everything is ready and the system can be used.</p>"},{"location":"developer/initial_setup/#6-running-an-experiment","title":"6. Running an Experiment","text":"<p>To test the setup, we can issue a query in the front end and explore some results. The clicks are logged by stella-search, send over to the stella-app, and then transfered to the stella-server where we can see the results on the dashboard. </p>"},{"location":"developer/introduction/","title":"Introduction","text":"<p>This is the developer introduction</p>"},{"location":"researcher/introduction/","title":"Introduction","text":"<p>Researchers conduct experiments in the stella infrastructure.</p>"},{"location":"researcher/participants-ranking/","title":"How to integrate a ranking micro-service","text":""},{"location":"researcher/participants-ranking/#1-introduction","title":"1. Introduction","text":"<p>The STELLA-framework enables participants to run and evaluate ranking and recommendation-systems in a Living-Lab scenario. The heart of STELLA-framework is the STELLA application, which is implemented as a a multi-container-application (MCA) by the sites (the search-engines) and handles communication between the site and participant-containers. This means every participant-system must be deployed as a docker image, which runs as a container inside STELLA application. In principle you have full freedom in choice of programming language and software tools. Your container only has to satisfy the constraints of a predefined REST-API. If you want to learn more about STELLA, please see our blog posts. This tutorial will guide you step-by-step through the process of integrating a ranking microservice into STELLA-infrastructure. You will learn how to build your own dockerized ranking-system with python. For this purpose we will use a document subset from the LIVIVO search engine. To give you a head start, we prepared a code template. That means, you just have to write a few lines of code to get your ranking-system running.</p>"},{"location":"researcher/participants-ranking/#2-prerequisites","title":"2. Prerequisites","text":"<p>Before starting this tutorial, make sure all requirements in the README.md are fulfilled.</p>"},{"location":"researcher/participants-ranking/#3-livivo-dataset","title":"3. LIVIVO Dataset","text":"<p>LIVIVO is an interdisciplinary search engine for literature and information in the field of life sciences. It is run by ZB MED \u2013 Information Centre for Life Sciences. LIVIVO draws on relevant scientific information from the ZB MED subject areas of medicine, health, nutrition, and environmental and agricultural sciences. (https://www.livivo.de/app/misc/help/about) In this tutorial we will work with a small subset of LIVIVO (30000 documents). See the following table for an explanation of fields.</p> field description DBRECORDID primary document key TITLE document title ABSTRACT document abstract AUTHOR list of authors INSTITUTION list of Institutions connected with the paper SOURCE publiction source VOLUME volume (for journal articles) ISSUE issue (for journal articles) PAGES page numbers referring to PUBLISHER document publisher LANGUAGE document language PUBLDATE publishing date PUBLYEAR publishing year PUBLPLACE publishing place PUBLCOUNTRY publishing country IDENTIFIER list of additional identifiers (pmid, pmcid, nlmid,...) DOI Document Object identifier ISSN International Standard Serial Number EISSN Electronic International Standard Serial Number PISSN Print International Standard Serial Number DATABASE Source Database from wich LIVIVO collected the document (MEDLINE,NLM,AGRIS,..) DOCUMENTURL URL for accessing document MESH list of MESH-terms KEYWORDS additional keywords CHEM list of chemical substances <p>The dataset is provided in jsonlines-format. Download the LIVIVO-testset here: https://th-koeln.sciebo.de/s/OBm0NLEwz1RYl9N/download?path=%2Flivivo%2Fdocuments&amp;files=livivo_testset.jsonl</p>"},{"location":"researcher/participants-ranking/#4-forking-and-cloning-stella-microservice-template","title":"4. Forking and cloning STELLA-microservice Template","text":"<p>Before you start working on your own ranking-system, you should fork our template. Navigate with your browser to our template repository https://github.com/stella-project/stella-micro-template and click the \"fork\"-button on the top right. This will create a fork in your personal github-account.  Now navigate to your working directory and run the following command to clone the forked repository to your local system.</p> <pre><code>$ git clone https://github.com/your-username/stella-micro-template.git\n</code></pre>"},{"location":"researcher/participants-ranking/#5-adding-dataset-to-your-local-environment","title":"5. Adding dataset to your local environment","text":"<p>Please download the LIVIVO-testset here: https://th-koeln.sciebo.de/s/OBm0NLEwz1RYl9N/download?path=%2Flivivo%2Fdocuments&amp;files=livivo_testset.jsonl Place the file into stella-micro-template/data/livivo/datasets. If these subfolders do not exist, create them.</p> <pre><code>\u251c\u2500\u2500 stella-micro-template\n\u2502   \u251c\u2500\u2500 data\n\u2502   \u2502   \u251c\u2500\u2500 livivo\n\u2502   \u2502       \u251c\u2500\u2500 documents\n\u2502   \u2502           \u251c\u2500\u2500 livivo_testset.jsonl\n</code></pre>"},{"location":"researcher/participants-ranking/#6-rest-endpoints-with-flask","title":"6. REST-Endpoints with Flask","text":"<p>To connect your system with STELLA, your container has to provide REST-endpoints according to the STELLA-interface. Your system should provide three endpoints. First of all, your system has to implement an indexing-endpoint. This endpoint is called when your system is used for the first time. It builds a search-index from the data provided by the site. The ranking-endpoint must return an ordered list of document-ids in JSON format. If you provide these endpoints, your results will be integrated seamlessly into the sites\u2019 pages. The test-endpoint is, as you may have guessed, for testing if your container is up and running. It just displays the name of your container.</p> <ul> <li> <p>GET <code>/index</code>   Starts indexing data, when starting ranking-system for the first time</p> </li> <li> <p>GET <code>/ranking?query=&lt;string:qstr&gt;&amp;page=&lt;int:pnum&gt;&amp;rpp=&lt;int:rppnum&gt;</code>   Returns an ordered list of document-ids in JSON-Format</p> </li> <li> <p>GET <code>/test</code>   This Endpoints is for testing purposes and just prints the name of your container</p> </li> </ul> <p>For a more detailled explanation of the API-Interface please visit our API-interface-documentation.</p> <p>All these endpoints are already existing in the template. Please have a look into the file <code>app.py</code>. Plese don't do any changes in that file.</p> <pre><code>from flask import Flask, request, jsonify, redirect\nfrom systems import Ranker, Recommender\n\n\napp = Flask(__name__)\nranker = Ranker()\nrecommender = Recommender()\n\n\n@app.route('/')\ndef redirect_to_test():\n    return redirect(\"/test\", code=302)\n\n\n@app.route('/test', methods=[\"GET\"])\ndef test():\n    return 'Container is running', 200\n\n\n@app.route('/index', methods=[\"GET\"])\ndef index():\n    ranker.index()\n    recommender.index()\n    return 'Indexing done!', 200\n\n\n@app.route('/ranking', methods=[\"GET\"])\ndef ranking():\n    query = request.args.get('query', None)\n    page = request.args.get('page', default=0, type=int)\n    rpp = request.args.get('rpp', default=20, type=int)\n    response = ranker.rank_publications(query, page, rpp)\n    return jsonify(response)\n\n\n@app.route('/recommendation/datasets', methods=[\"GET\"])\ndef rec_data():\n    item_id = request.args.get('item_id', None)\n    page = request.args.get('page', default=0, type=int)\n    rpp = request.args.get('rpp', default=20, type=int)\n    response = recommender.recommend_datasets(item_id, page, rpp)\n    return jsonify(response)\n\n\n@app.route('/recommendation/publications', methods=[\"GET\"])\ndef rec_pub():\n    item_id = request.args.get('item_id', None)\n    page = request.args.get('page', default=0, type=int)\n    rpp = request.args.get('rpp', default=20, type=int)\n    response = recommender.recommend_publications(item_id, page, rpp)\n    return jsonify(response)\n\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=5000, debug=True)\n</code></pre> <p>You might have registered some endpoints for building recommendation-systems. Since we are only building a ranking-system, please ignore these endpoints for now.</p>"},{"location":"researcher/participants-ranking/#7-indexing-data","title":"7. Indexing data","text":"<p>Let's start indexing the data from \"livivo_test.jsonl\". This file contains data in jsonlines format, which means, each line is an individual json-object, which represents a single document. Open the file <code>systems.py</code>. This file contains to classes: Ranker and Recommender. As the name says, the first one is for implementing a rankings, the second one for recommender-systems. In this tutorial we will only change the Ranker-Class.</p> <p>At first, we will implement the code for indexing documents. We will read the documents with the help of the jsonlines-package. Don't forget to import the package first.  For the sake of simplicity, we will just extract the unique identifier of any document and store them in the python built-in type \"list\". At a later stage, when you work with the full corpus of LIVIVO, which contains more than 60 mio documents, you should switch to a more sophisticated Index.  You will change the method <code>index(self)</code> so it reads the documents from <code>livivo_testset.jsonl</code></p> <p>The Ranker-Class in your file <code>systems.py</code> should now look like this:</p> <pre><code>import jsonlines\n\nclass Ranker(object):\n\n    def __init__(self):\n        self.idx = None\n\n    def index(self):\n        self.idx = []\n        with jsonlines.open('./data/livivo/documents/livivo_testset.jsonl') as reader:\n            for obj in reader:\n                self.idx.append(obj['DBRECORDID'])\n\n    def rank_publications(self, query, page, rpp):\n\n        itemlist = []\n\n        return {\n            'page': page,\n            'rpp': rpp,\n            'query': query,\n            'itemlist': itemlist,\n            'num_found': len(itemlist)\n        }\n</code></pre>"},{"location":"researcher/participants-ranking/#8-ranking-documents-shuffled","title":"8. ranking documents (shuffled)","text":"<p>Now we will implement our personal ranking-algorithm. To keep it simple, our ranking-algorithm picks just a number of random documents. This will give you an idea, how it works and enables you to implement some smart ranking-algorithms after this tutorial. For picking some random entries from our item-list, we can use the choice-method from the random package which is part of the python library and doese not need any installation (i.e. you do not need to add it to the requirements.txt file). You will be changing the first line of the method <code>rank_publications(self, query, page, rpp)</code> to get a randomized item list <code>itemlist = random.choices(self.idx, k=rpp)</code></p> <p>The Ranker-Class in your file <code>systems.py</code> should now look like this:</p> <pre><code>import jsonlines\nimport random\n\nclass Ranker(object):\n\n    def __init__(self):\n        self.idx = None\n\n    def index(self):\n        self.idx = []\n        with jsonlines.open('./data/livivo/documents/livivo_testset.jsonl') as reader:\n            for obj in reader:\n                self.idx.append(obj['DBRECORDID'])\n\n    def rank_publications(self, query, page, rpp):\n\n        itemlist = random.choices(self.idx, k=rpp)\n\n        return {\n            'page': page,\n            'rpp': rpp,\n            'query': query,\n            'itemlist': itemlist,\n            'num_found': len(itemlist)\n        }\n</code></pre> <p>If you decide to use external packages, please add them to the file <code>requirements.py</code>. In our example, we decided to use the package <code>jsonlines</code>. Your file <code>requirements.py</code> should now look like this</p> <pre><code>flask\njsonlines\n</code></pre>"},{"location":"researcher/participants-ranking/#9-setup-virtual-environment","title":"9. setup virtual environment","text":"<p>We are now ready to run your application. Before pushing your code to github, you should verify, that everything works as expected. To do so, you should create a virtual-environment, install the necessary dependencies there and run your Ranker on your local machine. Python IDEs (like PyCharm) offer functionalities to create a virtual python environment inside your project. Since we don't know which IDE you are using, we show you, how to create Virtual-Environment manually. Inside the directory of your cloned-repository, please run this command.</p> <p>On Linux and MacOS <pre><code>$ python3 -m venv venv\n</code></pre></p> <p>On Windows <pre><code>$ py -m venv venv\n</code></pre></p> <p>This creates a virtual python environment in the folder <code>venv</code>. To activate and work with your virtual python environment, please run this commadn:</p> <p>On Linux and MacOs <pre><code>$ source venv/bin/activate\n</code></pre></p> <p>On Windows <pre><code>.\\venv\\Scripts\\activate\n</code></pre></p> <p>Now, you have to make sure, that all necessary packages are available in your virtual python environment.</p> <p><pre><code>pip install -r requirements.txt\n</code></pre> Don't push the folder <code>venv</code> which contains the virtual python environment to your github-repository. It's just for testing your Ranker on your local machine. </p>"},{"location":"researcher/participants-ranking/#10-run-ranker-in-virtual-environment","title":"10. run ranker in virtual environment","text":"<p>In your activated virtual environment, start the application.</p> <pre><code>$ python app.py\n</code></pre> <p>You will get some messages on the shell, indicating, that your Flask-App is running. Now we can do some checks to see if everything works as it should.</p> <p>Please open your browser and access the following address http://0.0.0.0:5000/test (you might need to use http://localhost:5000/test in Windows). Your browser should reply with the message \"Container is running\". This means, Flask is running and serving at Port 5000. Do not worry if your try http://0.0.0.0:5000/ as that entry point is not implemented (i.e. it is expected that you will get an error).</p> <p>Now we want to start indexing documents with our ranker. Please access the following address with your browser http://0.0.0.0:5000/index (or http://localhost:5000/index in Windows). This may take some time, when indexing a big collection. After indexing was successful, your browser should replay with \"Indexing done!\".</p> <p>We are ready to send queries to our ranking-system and get results. Access the following address with your browser http://0.0.0.0:5000/ranking?query=test (you might need to use http://localhost:5000/ranking?query=test in Windows). Your browser will reply with a JSON-Document, that look like to this (itemlist will be different, because the results are randomized):</p> <pre><code>{\n  \"itemlist\": [\n    \"M7064959\", \n    \"NLM7513803\", \n    \"NLM8300642A\", \n    \"AGRISUS201600020713\", \n    \"AGRISFR2016217161\", \n    \"NLM26061140R\", \n    \"NLM101220129\", \n    \"M28128407\", \n    \"AGRISUS201400054734\", \n    \"NLM101070554\", \n    \"NLM101230052\", \n    \"M17665832\", \n    \"M28313204\", \n    \"NLM9412462\", \n    \"NLM101068566\", \n    \"AGRISJP2009005254\", \n    \"NLM101173006\", \n    \"M4793495\", \n    \"M2965586\", \n    \"M2082902\"\n  ], \n  \"num_found\": 20, \n  \"page\": 0, \n  \"query\": \"test\", \n  \"rpp\": 20\n}\n</code></pre> <p>Congratulations! Your ranker is running! We can now start to build a dockerized version and run unit-tests!</p>"},{"location":"researcher/participants-ranking/#11-build-docker-container-and-run-unit-tests","title":"11. Build docker-container and run unit-tests","text":"<p>You are now ready to build your ranker as a docker-container and run unit-tests to check if everything is working as expected, before pushing your code back to github.</p> <p>Navigate to the folder <code>test</code>. Make sure, you still have your virtual environment activated and stopped your ranking application. First, install the python-dependencies needed for running the unit-tests:</p> <pre><code>$ pip install -r requirements.txt\n</code></pre> <p>Next, we are ready to build and run your ranker as docker-container. Please run the python-script \"docker_build_run.py\".</p> <pre><code>$ python docker_build_run.py\n</code></pre> <p>Open your browser and please verifiy, if the endpoints are accessible: - http://0.0.0.0:5000/test (http://localhost:5000/test in Windows) - http://0.0.0.0:5000/index (http://localhost:5000/index in Windows) - http://0.0.0.0:5000/ranking?query=test (http://localhost:5000/ranking?query=test in Windows)</p> <p>Now we can run the unit-tests:</p> <pre><code>$ python test_ranking.py \n</code></pre> <p>This will run 6 different tests, which your container has to pass, before it can be integrated into STELLA The Script should print the message \"OK\". If you are using Windows, make sure you target 'localhost' rather than '0.0.0.0' on the IP variable in the test (i.e. <code>IP = 'localhost'</code> for Windows; for other OS <code>IP = '0.0.0.0.</code> should work).</p>"},{"location":"researcher/participants-ranking/#12-push-your-changes-to-github","title":"12. push your changes to github","text":"<p>Now you are ready to push your ranker back to github.</p> <p>Add the changed files to the the staging area.</p> <pre><code>git add systems.py requirements.txt\n</code></pre> <p>Save the changes to you local repository.</p> <pre><code>git commit -m \"add systems and requirements\"\n</code></pre> <p>Update remote branch with local commit.</p> <pre><code>git push origin main\n</code></pre> <p>Your ranker is now ready for integration into the STELLA-infrastructure. Please make sure, that the visibility of your repository is set to \"public\".</p>"},{"location":"researcher/participants-recommender-publications/","title":"How to integrate a micro-service for recommending publications","text":""},{"location":"researcher/participants-recommender-publications/#1-introduction","title":"1. Introduction","text":"<p>The STELLA-framework enables participants to run and evaluate ranking and recommender-systems in a Living-Lab scenario. The heart of STELLA-framework is the STELLA application, which is implemented as a a multi-container-application (MCA) by the sites (the search-engines) and handles communication between the site and participant-containers. This means every participant-system must be deployed as a docker image, which runs as a container inside STELLA application. In principle you have full freedom in choice of programming language and software tools. Your container only has to satisfy the constraints of a predefined REST-API. If you want to learn more about STELLA, please see our blog posts. This tutorial will guide you step-by-step through the process of integrating a recommendation microservice into STELLA-infrastructure. You will learn how to build your own dockerized recommender-system for publication with python. For this purpose we will use a document subset from the LIVIVO search engine. To give you a head start, we prepared a code template. That means, you just have to write a few lines of code to get your recommender-system running.</p>"},{"location":"researcher/participants-recommender-publications/#2-prerequisites","title":"2. Prerequisites","text":"<p>Before starting this tutorial, make sure all requirements in the README.md are fulfilled.</p>"},{"location":"researcher/participants-recommender-publications/#3-livivo-dataset","title":"3. LIVIVO Dataset","text":"<p>LIVIVO is an interdisciplinary search engine for literature and information in the field of life sciences. It is run by ZB MED \u2013 Information Centre for Life Sciences. LIVIVO draws on relevant scientific information from the ZB MED subject areas of medicine, health, nutrition, and environmental and agricultural sciences. (https://www.livivo.de/app/misc/help/about) In this tutorial we will work with a small subset of LIVIVO (30000 documents). See the following table for an explanation of fields.</p> field description DBRECORDID primary document key TITLE document title ABSTRACT document abstract AUTHOR list of authors INSTITUTION list of Institutions connected with the paper SOURCE publiction source VOLUME volume (for journal articles) ISSUE issue (for journal articles) PAGES page numbers referring to PUBLISHER document publisher LANGUAGE document language PUBLDATE publishing date PUBLYEAR publishing year PUBLPLACE publishing place PUBLCOUNTRY publishing country IDENTIFIER list of additional identifiers (pmid, pmcid, nlmid,...) DOI Document Object identifier ISSN International Standard Serial Number EISSN Electronic International Standard Serial Number PISSN Print International Standard Serial Number DATABASE Source Database from wich LIVIVO collected the document (MEDLINE,NLM,AGRIS,..) DOCUMENTURL URL for accessing document MESH list of MESH-terms KEYWORDS additional keywords CHEM list of chemical substances <p>The dataset is provided in jsonlines-format. Download the LIVIVO-testset here: LIVIVO test-set</p>"},{"location":"researcher/participants-recommender-publications/#4-forking-and-cloning-stella-microservice-template","title":"4. Forking and cloning STELLA-microservice Template","text":"<p>Before you start working on your own recommender-system, you should fork our template. Navigate with your browser to our template repository https://github.com/stella-project/stella-micro-template and click the \"fork\"-button on the top right. This will create a fork in your personal github-account.  Now navigate to your working directory and run the following command to clone the forked repository to your local system.</p> <pre><code>$ git clone https://github.com/your-username/stella-micro-template.git\n</code></pre>"},{"location":"researcher/participants-recommender-publications/#5-adding-dataset-to-your-local-environment","title":"5. Adding dataset to your local environment","text":"<p>Please download the LIVIVO-testset here: LIVIVO test-set Place the file into stella-micro-template/data/livivo/datasets. If these subfolders do not exist, create them.</p> <pre><code>\u251c\u2500\u2500 stella-micro-template\n\u2502   \u251c\u2500\u2500 data\n\u2502   \u2502   \u251c\u2500\u2500 livivo\n\u2502   \u2502       \u251c\u2500\u2500 documents\n\u2502   \u2502           \u251c\u2500\u2500 livivo_testset.jsonl\n</code></pre>"},{"location":"researcher/participants-recommender-publications/#6-rest-endpoints-with-flask","title":"6. REST-Endpoints with Flask","text":"<p>To connect your system with STELLA, your container has to provide REST-endpoints according to the STELLA-interface. Your system should provide three endpoints. First of all, your system has to implement an indexing-endpoint. This endpoint is called when your system is used for the first time. It builds a search-index from the data provided by the site. The recommendation-endpoint must return an ordered list of document-ids in JSON format. If you provide these endpoints, your results will be integrated seamlessly into the sites\u2019 pages. The test-endpoint is, as you may have guessed, for testing if your container is up and running. It just displays the name of your container.</p> <ul> <li> <p>GET <code>/index</code>   Starts indexing data, when starting ranking-system for the first time</p> </li> <li> <p>GET <code>/recommendadtion/publications?item_id=&lt;string:item_id&gt;&amp;page=&lt;int:pnum&gt;&amp;rpp=&lt;int:rppnum&gt;</code>   Returns an ordered list of document-ids in JSON-Format</p> </li> <li> <p>GET <code>/test</code>   This Endpoints is for testing purposes and just prints the name of your container</p> </li> </ul> <p>For a more detailled explanation of the API-Interface please visit our API-interface-documentation.</p> <p>All these endpoints are already existing in the template. Please have a look into the file <code>app.py</code>. Plese don't do any changes in that file.</p> <pre><code>from flask import Flask, request, jsonify, redirect\nfrom systems import Ranker, Recommender\n\n\napp = Flask(__name__)\nranker = Ranker()\nrecommender = Recommender()\n\n\n@app.route('/')\ndef redirect_to_test():\n    return redirect(\"/test\", code=302)\n\n\n@app.route('/test', methods=[\"GET\"])\ndef test():\n    return 'Container is running', 200\n\n\n@app.route('/index', methods=[\"GET\"])\ndef index():\n    ranker.index()\n    recommender.index()\n    return 'Indexing done!', 200\n\n\n@app.route('/ranking', methods=[\"GET\"])\ndef ranking():\n    query = request.args.get('query', None)\n    page = request.args.get('page', default=0, type=int)\n    rpp = request.args.get('rpp', default=20, type=int)\n    response = ranker.rank_publications(query, page, rpp)\n    return jsonify(response)\n\n\n@app.route('/recommendation/datasets', methods=[\"GET\"])\ndef rec_data():\n    item_id = request.args.get('item_id', None)\n    page = request.args.get('page', default=0, type=int)\n    rpp = request.args.get('rpp', default=20, type=int)\n    response = recommender.recommend_datasets(item_id, page, rpp)\n    return jsonify(response)\n\n\n@app.route('/recommendation/publications', methods=[\"GET\"])\ndef rec_pub():\n    item_id = request.args.get('item_id', None)\n    page = request.args.get('page', default=0, type=int)\n    rpp = request.args.get('rpp', default=20, type=int)\n    response = recommender.recommend_publications(item_id, page, rpp)\n    return jsonify(response)\n\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=5000, debug=True)\n</code></pre> <p>You might have registered some endpoints for building ranking-systems. Since we are only building a recommendation-system, please ignore these endpoints for now.</p>"},{"location":"researcher/participants-recommender-publications/#7-indexing-data","title":"7. Indexing data","text":"<p>Let's start indexing the data from \"livivo_test.jsonl\". This file contains data in jsonlines format, which means, each line is an individual json-object, which represents a single document. Open the file <code>systems.py</code>. This file contains two classes: Ranker and Recommender. As the name says, the first one is for implementing a ranking, the second one for recommender-systems. In this tutorial we will only change the Recommender-Class.</p> <p>At first, we will implement the code for indexing documents. We will read the documents with the help of the jsonlines-package. Don't forget to import the package first.  For the sake of simplicity, we will just extract the unique identifier of any document and store them in the python built-in type \"list\". At a later stage, when you work with the full corpus of LIVIVO, which contains more than 60 mio documents, you should switch to a more sophisticated index.  You will change the method <code>index(self)</code> so it reads the documents from <code>livivo_testset.jsonl</code></p> <p>The Ranker-Class in your file <code>systems.py</code> should now look like this:</p> <pre><code>import jsonlines\n\nclass Recommender(object):\n\n    def __init__(self):\n        self.idx = None\n\n    def index(self):\n        self.idx = []\n        with jsonlines.open('./data/livivo/documents/livivo_testset.jsonl') as reader:\n            for obj in reader:\n                self.idx.append(obj['DBRECORDID'])\n\n    def recommend_datasets(self, item_id, page, rpp):\n\n        itemlist = []\n\n        return {\n            'page': page,\n            'rpp': rpp,\n            'item_id': item_id,\n            'itemlist': itemlist,\n            'num_found': len(itemlist)\n        }\n\n    def recommend_publications(self, item_id, page, rpp):\n\n        itemlist = []\n\n        return {\n            'page': page,\n            'rpp': rpp,\n            'item_id': item_id,\n            'itemlist': itemlist,\n            'num_found': len(itemlist)\n        }\n</code></pre>"},{"location":"researcher/participants-recommender-publications/#8-recommend-documents-shuffled","title":"8. recommend documents (shuffled)","text":"<p>Now we will implement our personal algorithm for recommending publications. To keep it simple, our recommender-algorithm picks just a number of random documents. This will give you an idea, how it works and enables you to implement some smart recommender-algorithms after this tutorial. For picking some random entries from our item-list, we can use the choice-method from the random package which is part of the python library and does not need any installation (i.e. you do not need to add it to the requirements.txt file). You will be changing the first line of the method <code>recommend_publications(self, query, page, rpp)</code> to get a randomized item list <code>itemlist = random.choices(self.idx, k=rpp)</code></p> <p>The Recommender-Cass in your file <code>systems.py</code> should now look like this:</p> <pre><code>import jsonlines\nimport random\n\nclass Recommender(object):\n\n    def __init__(self):\n        self.idx = None\n\n    def index(self):\n        self.idx = []\n        with jsonlines.open('./data/livivo/documents/livivo_testset.jsonl') as reader:\n            for obj in reader:\n                self.idx.append(obj['DBRECORDID'])\n\n    def recommend_datasets(self, item_id, page, rpp):\n\n        itemlist = []\n\n        return {\n            'page': page,\n            'rpp': rpp,\n            'item_id': item_id,\n            'itemlist': itemlist,\n            'num_found': len(itemlist)\n        }\n\n    def recommend_publications(self, item_id, page, rpp):\n\n        itemlist = random.choices(self.idx, k=rpp)\n\n        return {\n            'page': page,\n            'rpp': rpp,\n            'item_id': item_id,\n            'itemlist': itemlist,\n            'num_found': len(itemlist)\n        }\n</code></pre> <p>If you decide to use external packages, please add them to the file <code>requirements.py</code>. In our example, we decided to use the package <code>jsonlines</code>. Your file <code>requirements.py</code> should now look like this</p> <pre><code>flask\njsonlines\n</code></pre>"},{"location":"researcher/participants-recommender-publications/#9-setup-virtual-environment","title":"9. setup virtual environment","text":"<p>We are now ready to run your application. Before pushing your code to github, you should verify, that everything works as expected. To do so, you should create a virtual-environment, install the necessary dependencies there and run your Ranker on your local machine. Python IDEs (like PyCharm) offer functionalities to create a virtual python environment inside your project. Since we don't know which IDE you are using, we show you, how to create Virtual-Environment manually. Inside the directory of your cloned-repository, please run this command.</p> <p>On Linux and MacOS <pre><code>$ python3 -m venv venv\n</code></pre></p> <p>On Windows <pre><code>$ py -m venv venv\n</code></pre></p> <p>This creates a virtual python environment in the folder <code>venv</code>. To activate and work with your virtual python environment, please run this commadn:</p> <p>On Linux and MacOs <pre><code>$ source venv/bin/activate\n</code></pre></p> <p>On Windows <pre><code>.\\venv\\Scripts\\activate\n</code></pre></p> <p>Now, you have to make sure, that all necessary packages are available in your virtual python environment.</p> <p><pre><code>pip install -r requirements.txt\n</code></pre> Don't push the folder <code>venv</code> which contains the virtual python environment to your github-repository. It's just for testing your Recommender on your local machine. </p>"},{"location":"researcher/participants-recommender-publications/#10-run-ranker-in-virtual-environment","title":"10. run ranker in virtual environment","text":"<p>In your activated virtual environment, start the application.</p> <pre><code>$ python app.py\n</code></pre> <p>You will get some messages on the shell, indicating, that your Flask-App is running. Now we can do some checks to see if everything works as it should.</p> <p>Please open your browser and access the following address http://0.0.0.0:5000/test (you might need to use http://localhost:5000/test in Windows). Your browser should reply with the message \"Container is running\". This means, Flask is running and serving at Port 5000. Do not worry if your try http://0.0.0.0:5000/ as that entry point is not implemented (i.e. it is expected that you will get an error).</p> <p>Now we want to start indexing documents with our recommender. Please access the following address with your browser http://0.0.0.0:5000/index (or http://localhost:5000/index in Windows). This may take some time, when indexing a big collection. After indexing was successful, your browser should replay with \"Indexing done!\".</p> <p>We are ready to send queries to our ranking-system and get results. Access the following address with your browser http://0.0.0.0:5000/recommendation/publication?item_id=M5339676 (you might need to use http://localhost:5000/ranking?query=test in Windows). Please make sure using an item_id that exists in the indexed dataset, otherwise you'll may not get any results. Your browser will reply with a JSON-Document, that looks like this (itemlist will be different, because the results are randomized):</p> <pre><code>{\n  \"itemlist\": [\n    \"M7064959\", \n    \"NLM7513803\", \n    \"NLM8300642A\", \n    \"AGRISUS201600020713\", \n    \"AGRISFR2016217161\", \n    \"NLM26061140R\", \n    \"NLM101220129\", \n    \"M28128407\", \n    \"AGRISUS201400054734\", \n    \"NLM101070554\", \n    \"NLM101230052\", \n    \"M17665832\", \n    \"M28313204\", \n    \"NLM9412462\", \n    \"NLM101068566\", \n    \"AGRISJP2009005254\", \n    \"NLM101173006\", \n    \"M4793495\", \n    \"M2965586\", \n    \"M2082902\"\n  ], \n  \"num_found\": 20, \n  \"page\": 0, \n  \"query\": \"test\", \n  \"rpp\": 20\n}\n</code></pre> <p>Congratulations! Your ranker is running! We can now start to build a dockerized version and run unit-tests!</p>"},{"location":"researcher/participants-recommender-publications/#11-build-docker-container-and-run-unit-tests","title":"11. Build docker-container and run unit-tests","text":"<p>You are now ready to build your ranker as a docker-container and run unit-tests to check if everything is working as expected, before pushing your code back to github.</p> <p>Navigate to the folder <code>test</code>. Make sure, you still have your virtual environment activated and stopped your ranking application. First, install the python-dependencies needed for running the unit-tests:</p> <pre><code>$ pip install -r requirements.txt\n</code></pre> <p>Next, we are ready to build and run your ranker as docker-container. Please run the python-script \"docker_build_run.py\".</p> <pre><code>$ python docker_build_run.py\n</code></pre> <p>Open your browser and please verifiy, if the endpoints are accessible: - http://0.0.0.0:5000/test (http://localhost:5000/test in Windows) - http://0.0.0.0:5000/index (http://localhost:5000/index in Windows) - http://0.0.0.0:5000/recommendation/publications?item_id=M8498126 (http://localhost:5000/recommendation/publications?item_id=M8498126 in Windows)</p> <p>Now we can run the unit-tests:</p> <pre><code>$ python test_recommendation_publications.py \n</code></pre> <p>This will run 6 different tests, which your container has to pass, before it can be integrated into STELLA The Script should print the message \"OK\". If you are using Windows, make sure you target 'localhost' rather than '0.0.0.0' on the IP variable in the test (i.e. <code>IP = 'localhost'</code> for Windows; for other OS <code>IP = '0.0.0.0.</code> should work).</p>"},{"location":"researcher/participants-recommender-publications/#12-push-your-changes-to-github","title":"12. push your changes to github","text":"<p>Now you are ready to push your ranker back to github.</p> <p>Add the changed files to the the staging area.</p> <pre><code>git add systems.py requirements.txt\n</code></pre> <p>Save the changes to you local repository.</p> <pre><code>git commit -m \"add systems and requirements\"\n</code></pre> <p>Update remote branch with local commit.</p> <pre><code>git push origin main\n</code></pre> <p>Your recommender-system is now ready for integration into the STELLA-infrastructure. Please make sure, that the visibility of your repository is set to \"public\".</p>"},{"location":"resources/resources/","title":"Resources","text":""},{"location":"resources/resources/#resources-for-participants","title":"Resources for Participants","text":"<ul> <li>YouTube - LiLAS@CLEF21: Implementing a recommendation service</li> <li>GitHub - Micro template</li> </ul>"},{"location":"resources/resources/#github-repositories","title":"GitHub Repositories","text":"<ul> <li> <p>Main Website</p> </li> <li> <p>STELLA Project (GitHub Organization)</p> </li> <li> <p>STELLA App</p> </li> <li> <p>STELLA Server</p> </li> <li> <p>STELLA Microservice Template</p> </li> <li> <p>STELLA Search (Mock Search Service)</p> </li> </ul>"},{"location":"resources/resources/#lilas-lab","title":"Lilas-Lab","text":"<p>The Lilas-Lab was a CLEF lab held in 2021 that employed STELLA for Living-Lab Experiments.</p>"},{"location":"site-owner/introduction/","title":"Introduction","text":"<p>This is the introduction page for the site owner</p>"},{"location":"site-owner/mca/","title":"STELLA-App","text":""},{"location":"site-owner/mca/#ranking","title":"Ranking","text":""},{"location":"site-owner/mca/#rest-endpoint","title":"REST endpoint:","text":"<p>GET <code>/stella/api/v1/ranking?query=&lt;string:query&gt;&amp;page=&lt;int:page&gt;&amp;rpp=&lt;int:rpp&gt;&amp;sid=&lt;int:sid&gt;&amp;container=&lt;string:container&gt;</code> </p>"},{"location":"site-owner/mca/#explanation","title":"Explanation:","text":"<ul> <li>query: the query string</li> <li>page: the number of the start page (optional)</li> <li>rpp: the number of results per page (optional)</li> <li>container: name of the container that contains either the baseline or one of the experimental systems (optional)</li> <li>sid: the session identifier (optional)</li> </ul>"},{"location":"site-owner/mca/#output-interleaved-ranking","title":"Output (interleaved ranking):","text":"<pre><code>{'body': {'1': {'docid': 'M27622217', 'type': 'BASE'},\n          '2': {'docid': 'M27251231', 'type': 'EXP'},\n          '3': {'docid': 'M27692969', 'type': 'BASE'},\n          '4': {'docid': 'M26350569', 'type': 'EXP'},\n          '5': {'docid': 'M26715777', 'type': 'EXP'},\n          '6': {'docid': 'M26650940', 'type': 'BASE'},\n          '7': {'docid': 'M27098271', 'type': 'EXP'},\n          '8': {'docid': 'M28381438', 'type': 'BASE'},\n          '9': {'docid': 'M27763523', 'type': 'EXP'},\n          '10': {'docid': 'M27157745', 'type': 'BASE'},\n          '11': {'docid': 'M28066266', 'type': 'EXP'},\n          '12': {'docid': 'M26874427', 'type': 'BASE'},\n          '13': {'docid': 'M27133457', 'type': 'EXP'},\n          '14': {'docid': 'M26791355', 'type': 'BASE'},\n          '15': {'docid': 'M27157753', 'type': 'BASE'},\n          '16': {'docid': 'M27167258', 'type': 'EXP'},\n          '17': {'docid': 'M27524068', 'type': 'EXP'},\n          '18': {'docid': 'M26824628', 'type': 'BASE'},\n          '19': {'docid': 'M26967532', 'type': 'EXP'}},\n 'header': {'container': {'base': 'rank_elastic_base', 'exp': 'rank_elastic'},\n            'page': 0,\n            'q': 'vaccine',\n            'rid': 3,\n            'rpp': 20,\n            'hits': 12312,\n            'sid': 1}}\n</code></pre>"},{"location":"site-owner/mca/#explanation_1","title":"Explanation:","text":"<ul> <li>header: header containing meta information about the returned result</li> <li>body: body with rank positions, identifiers, and type of the corresponding system</li> <li>docid: the document identifier</li> <li>type: type of system can be either <code>BASE</code> or <code>EXP</code></li> <li>container: dictionary with names of the experimental system and optional baseline system</li> <li>page: the number of the start page</li> <li>q: the query string</li> <li>rid: the ranking identifier</li> <li>rpp: the number of results per page</li> <li>hits: the number of total hits</li> <li>sid: the session identifier</li> </ul>"},{"location":"site-owner/mca/#output-non-interleaved-ranking","title":"Output (non-interleaved ranking):","text":"<pre><code>{'body': {'1': {'docid': 'M27622217', 'type': 'EXP'},\n          '2': {'docid': 'M27251231', 'type': 'EXP'},\n          '3': {'docid': 'M27692969', 'type': 'EXP'},\n          '4': {'docid': 'M26350569', 'type': 'EXP'},\n          '5': {'docid': 'M26715777', 'type': 'EXP'},\n          '6': {'docid': 'M26650940', 'type': 'EXP'},\n          '7': {'docid': 'M27098271', 'type': 'EXP'},\n          '8': {'docid': 'M28381438', 'type': 'EXP'},\n          '9': {'docid': 'M27763523', 'type': 'EXP'},\n          '10': {'docid': 'M27157745', 'type': 'EXP'},\n          '11': {'docid': 'M28066266', 'type': 'EXP'},\n          '12': {'docid': 'M26874427', 'type': 'EXP'},\n          '13': {'docid': 'M27133457', 'type': 'EXP'},\n          '14': {'docid': 'M26791355', 'type': 'EXP'},\n          '15': {'docid': 'M27157753', 'type': 'EXP'},\n          '16': {'docid': 'M27167258', 'type': 'EXP'},\n          '17': {'docid': 'M27524068', 'type': 'EXP'},\n          '18': {'docid': 'M26824628', 'type': 'EXP'},\n          '19': {'docid': 'M26967532', 'type': 'EXP'}},\n 'header': {'container': {'exp': 'rank_elastic'},\n            'page': 0,\n            'q': 'vaccine',\n            'rid': 1,\n            'rpp': 20,\n            'hits': 12312,\n            'sid': 1}}\n</code></pre>"},{"location":"site-owner/mca/#explanation_2","title":"Explanation:","text":"<p>See above.</p>"},{"location":"site-owner/mca/#feedback","title":"Feedback","text":""},{"location":"site-owner/mca/#rest-endpoint_1","title":"REST endpoint:","text":"<p>POST <code>/stella/api/v1/ranking/rid=&lt;int:rid&gt;/feedback</code></p>"},{"location":"site-owner/mca/#explanation_3","title":"Explanation:","text":"<ul> <li>sid: the session identifier</li> <li>rid: the ranking identifier</li> </ul>"},{"location":"site-owner/mca/#payload","title":"Payload:","text":"<pre><code>{'clicks': {'1': {'clicked': False,\n                  'date': None,\n                  'docid': 'M26923455',\n                  'type': 'EXP'},\n            '2': {'clicked': False,\n                  'date': None,\n                  'docid': 'M25600519',\n                  'type': 'EXP'},\n            '3': {'clicked': True,\n                  'date': '2020-07-29 16:06:51',\n                  'docid': 'M27515393',\n                  'type': 'EXP'},\n            '4': {'clicked': False,\n                  'date': None,\n                  'docid': 'M27572122',\n                  'type': 'EXP'},\n            '5': {'clicked': False,\n                  'date': None,\n                  'docid': 'M27357208',\n                  'type': 'EXP'},\n            '6': {'clicked': True,\n                  'date': '2020-07-29 16:06:51',\n                  'docid': 'M27309042',\n                  'type': 'EXP'},\n            '7': {'clicked': False,\n                  'date': None,\n                  'docid': 'M27237391',\n                  'type': 'EXP'},\n            '8': {'clicked': False,\n                  'date': None,\n                  'docid': 'M27279275',\n                  'type': 'EXP'},\n            '9': {'clicked': False,\n                  'date': None,\n                  'docid': 'M26813237',\n                  'type': 'EXP'}\n            '10': {'clicked': False,\n                   'date': None,\n                   'docid': 'M27049797',\n                   'type': 'EXP'},\n            '11': {'clicked': False,\n                   'date': None,\n                   'docid': 'M27531820',\n                   'type': 'EXP'},\n            '12': {'clicked': False,\n                   'date': None,\n                   'docid': 'M27338346',\n                   'type': 'EXP'},\n            '13': {'clicked': False,\n                   'date': None,\n                   'docid': 'M27999240',\n                   'type': 'EXP'},\n            '14': {'clicked': False,\n                   'date': None,\n                   'docid': 'M26613600',\n                   'type': 'EXP'},\n            '15': {'clicked': False,\n                   'date': None,\n                   'docid': 'M27356552',\n                   'type': 'EXP'},\n            '16': {'clicked': False,\n                   'date': None,\n                   'docid': 'M27783754',\n                   'type': 'EXP'},\n            '17': {'clicked': False,\n                   'date': None,\n                   'docid': 'M27278100',\n                   'type': 'EXP'},\n            '18': {'clicked': False,\n                   'date': None,\n                   'docid': 'M27531823',\n                   'type': 'EXP'},\n            '19': {'clicked': False,\n                   'date': None,\n                   'docid': 'M26860287',\n                   'type': 'EXP'}},\n 'end': '2020-07-29 16:12:53',\n 'interleave': True,\n 'start': '2020-07-29 16:06:51'}\n</code></pre>"},{"location":"site-owner/mca/#explanation_4","title":"Explanation:","text":"<ul> <li>start: time of session start, has to be provided as datetime-formatted string like 'YYYY-MM-DD HH:MM:SS'</li> <li>interleave: boolean value indicating if the result list has been interleaved</li> <li>end: time of session end, has to be provided as datetime-formatted string like 'YYYY-MM-DD HH:MM:SS'</li> <li>clicks: dicitionary containing document identifiers and their corresponding clicks</li> <li>clicked: boolean values indicating whether document has been clicked or not</li> <li>date: time when click happend, has to be provided as datetime-formatted string like 'YYYY-MM-DD HH:MM:SS'</li> <li>docid: the document identifier</li> <li>system: string value indicating if the corresponding system is baseline (<code>BASE</code>) or experimental (<code>EXP</code>)</li> </ul>"},{"location":"site-owner/mca/#recommendations","title":"Recommendations","text":""},{"location":"site-owner/mca/#rest-endpoint_2","title":"REST endpoint:","text":""},{"location":"site-owner/mca/#datasets","title":"Datasets","text":"<p>GET <code>/stella/api/v1/recommendation/datasets?itemid=&lt;string:itemid&gt;&amp;page=&lt;int:page&gt;&amp;rpp=&lt;int:rpp&gt;&amp;sid=&lt;int:sid&gt;&amp;container=&lt;string:container&gt;</code> </p>"},{"location":"site-owner/mca/#explanation_5","title":"Explanation:","text":"<ul> <li>itemid: the target item of the recommendations</li> <li>page: the number of the start page (optional)</li> <li>rpp: the number of results per page (optional)</li> <li>sid: the session identifier (optional)</li> <li>container: name of the container that contains either the baseline or one of the experimental systems (optional)</li> </ul>"},{"location":"site-owner/mca/#publications","title":"Publications","text":"<p>GET <code>/stella/api/v1/recommendation/publications?itemid=&lt;string:itemid&gt;&amp;page=&lt;int:page&gt;&amp;rpp=&lt;int:rpp&gt;&amp;sid=&lt;int:sid&gt;&amp;container=&lt;string:container&gt;</code> </p>"},{"location":"site-owner/mca/#explanation_6","title":"Explanation:","text":"<p>See above.</p>"},{"location":"site-owner/mca/#output-interleaved-recommendation","title":"Output (interleaved recommendation):","text":"<pre><code>{'body': {'1': {'docid': 'M27388739', 'type': 'BASE'},\n          '2': {'docid': 'M27338344', 'type': 'EXP'},\n          '3': {'docid': 'M26207529', 'type': 'EXP'},\n          '4': {'docid': 'M27216378', 'type': 'BASE'},\n          '5': {'docid': 'M26938486', 'type': 'BASE'},\n          '6': {'docid': 'M26612070', 'type': 'EXP'},\n          '7': {'docid': 'M27641359', 'type': 'BASE'},\n          '8': {'docid': 'M27567522', 'type': 'EXP'},\n          '9': {'docid': 'M27897418', 'type': 'BASE'}},\n 'header': {'container': {'base': 'recom_tfidf_base', 'exp': 'recom_tfidf'},\n            'itemid': 'M26923455',\n            'page': 0,\n            'rid': 4,\n            'rpp': 10,\n            'hits': 24,\n            'sid': 2,\n            'type': 'PUB'}}\n</code></pre>"},{"location":"site-owner/mca/#explanation_7","title":"Explanation:","text":"<ul> <li>header: header containing meta information about the returned result</li> <li>body: body with positions, identifiers, and type of the corresponding system</li> <li>docid: the document identifier</li> <li>type: type of system can be either <code>BASE</code> or <code>EXP</code></li> <li>container: dictionary with names of the experimental system and optional baseline system</li> <li>itemid: the target item of the recommendations</li> <li>page: the number of the start page</li> <li>rid: the recommendation identifier</li> <li>rpp: the number of results per page</li> <li>hits: the number of total hits</li> <li>sid: the session identifier</li> <li>type: type of system can be either <code>BASE</code> or <code>EXP</code></li> </ul>"},{"location":"site-owner/mca/#output-non-interleaved-recommendation","title":"Output (non-interleaved recommendation):","text":"<pre><code>{'body': {'1': {'docid': 'M27038470', 'type': 'EXP'},\n          '2': {'docid': 'M27342969', 'type': 'EXP'},\n          '3': {'docid': 'M26774951', 'type': 'EXP'},\n          '4': {'docid': 'M27912945', 'type': 'EXP'},\n          '5': {'docid': 'M26797943', 'type': 'EXP'},\n          '6': {'docid': 'M25359468', 'type': 'EXP'},\n          '7': {'docid': 'M26969740', 'type': 'EXP'},\n          '8': {'docid': 'M27613427', 'type': 'EXP'},\n          '9': {'docid': 'M27976545', 'type': 'EXP'}},\n 'header': {'container': {'exp': 'recom_tfidf'},\n            'itemid': 'M26923455',\n            'page': 0,\n            'rid': 2,\n            'rpp': 10,\n            'hits': 11,\n            'sid': 2,\n            'type': 'PUB'}}\n</code></pre>"},{"location":"site-owner/mca/#explanation_8","title":"Explanation:","text":"<p>See above.</p>"},{"location":"site-owner/mca/#feedback_1","title":"Feedback","text":""},{"location":"site-owner/mca/#rest-endpoint_3","title":"REST endpoint:","text":"<p>POST <code>/stella/api/v1/recommendation/rid=rid=&lt;int:rid&gt;/feedback</code></p>"},{"location":"site-owner/mca/#explanation_9","title":"Explanation:","text":"<ul> <li>rid: the recommendation identifier</li> </ul> <pre><code>{'clicks': {'1': {'clicked': False,\n                  'date': None,\n                  'docid': 'M27160449',\n                  'type': 'EXP'},\n            '2': {'clicked': False,\n                  'date': None,\n                  'docid': 'M27888935',\n                  'type': 'BASE'},\n            '3': {'clicked': False,\n                  'date': None,\n                  'docid': 'M27088628',\n                  'type': 'EXP'},\n            '4': {'clicked': False,\n                  'date': None,\n                  'docid': 'M27064543',\n                  'type': 'BASE'},\n            '5': {'clicked': False,\n                  'date': None,\n                  'docid': 'M27717979',\n                  'type': 'EXP'},\n            '6': {'clicked': True,\n                  'date': '2020-07-29 17:11:18',\n                  'docid': 'M27077760',\n                  'type': 'BASE'},\n            '7': {'clicked': False,\n                  'date': None,\n                  'docid': 'M27638054',\n                  'type': 'BASE'},\n            '8': {'clicked': False,\n                  'date': None,\n                  'docid': 'M26360828',\n                  'type': 'EXP'},\n            '9': {'clicked': False,\n                  'date': None,\n                  'docid': 'M27554937',\n                  'type': 'BASE'}},\n 'end': '2020-07-29 17:59:24',\n 'interleave': True,\n 'start': '2020-07-29 17:11:18'}\n</code></pre>"},{"location":"site-owner/mca/#explanation_10","title":"Explanation:","text":"<ul> <li>start: time of session start, has to be provided as datetime-formatted string like 'YYYY-MM-DD HH:MM:SS'</li> <li>interleave: boolean value indicating if the result list has been interleaved</li> <li>end: time of session end, has to be provided as datetime-formatted string like 'YYYY-MM-DD HH:MM:SS'</li> <li>clicks: dicitionary containing document identifiers and their corresponding clicks</li> <li>clicked: boolean values indicating whether document has been clicked or not</li> <li>date: time when click happend, has to be provided as datetime-formatted string like 'YYYY-MM-DD HH:MM:SS'</li> <li>docid: the document identifier</li> <li>system: string value indicating if the corresponding system is baseline (<code>BASE</code>) or experimental (<code>EXP</code>)</li> </ul>"},{"location":"site-owner/mca/#indexing","title":"Indexing","text":"<p>GET <code>/stella/api/v1/index/bulk</code></p>"},{"location":"site-owner/mca/#explanation_11","title":"Explanation:","text":"<p>Start indexing all containers in parallel.</p> <p>GET <code>/stella/api/v1/index/&lt;string:container_name&gt;</code></p>"},{"location":"site-owner/mca/#explanation_12","title":"Explanation:","text":"<ul> <li>container_name: Name of the specific container to be indexed.</li> </ul>"},{"location":"site-owner/mca/#exit-sessions","title":"Exit sessions","text":"<p>GET <code>/stella/api/v1/sessions/&lt;int:sid&gt;/exit</code></p>"},{"location":"site-owner/mca/#explanation_13","title":"Explanation:","text":"<ul> <li>sid: Identifier of the session to be exited.</li> </ul>"},{"location":"site-owner/micro-service/","title":"Micro-service","text":""},{"location":"site-owner/micro-service/#ranking","title":"Ranking","text":""},{"location":"site-owner/micro-service/#rest-endpoint","title":"REST endpoint:","text":"<p>GET <code>container_name/ranking?query=&lt;string:qstr&gt;&amp;page=&lt;int:pnum&gt;&amp;rpp=&lt;int:rppnum&gt;</code></p>"},{"location":"site-owner/micro-service/#explanation","title":"Explanation:","text":"<ul> <li>container_name: name of the container that contains either the baseline or one of the experimental systems</li> <li>query: the query string</li> <li>page: the number of the start page</li> <li>rpp: the number of results per page</li> </ul>"},{"location":"site-owner/micro-service/#output","title":"Output:","text":"<pre><code>{'itemlist': ['M26721328',\n              'M26923455',\n              'M25600519',\n              'M27515393',\n              'M27572122',\n              'M27357208',\n              'M27309042',\n              'M27237391',\n              'M27279275',\n              'M26813237',\n              'M27049797',\n              'M27531820',\n              'M27338346',\n              'M27999240',\n              'M26613600',\n              'M27356552',\n              'M27783754',\n              'M27278100',\n              'M27531823',\n              'M26860287'],\n 'num_found': 20,\n 'page': 0,\n 'query': 'vaccine',\n 'rpp': 20}\n</code></pre>"},{"location":"site-owner/micro-service/#explanation_1","title":"Explanation:","text":"<ul> <li>itemlist: a list containing the document identifiers</li> <li>num_found: the total number of documents found for the given <code>query</code></li> <li>page: the number of the start page</li> <li>query: the query string</li> <li>rpp: the number of results per page</li> </ul>"},{"location":"site-owner/micro-service/#recommendation","title":"Recommendation","text":""},{"location":"site-owner/micro-service/#rest-endpoint_1","title":"REST endpoint:","text":"<p>GET <code>container_name/recommendation/datasets?itemid=&lt;string:itemidstr&gt;&amp;page=&lt;int:pnum&gt;&amp;rpp=&lt;int:rppnum&gt;</code> GET <code>container_name/recommendation/publications?itemid=&lt;string:itemidstr&gt;&amp;page=&lt;int:pnum&gt;&amp;rpp=&lt;int:rppnum&gt;</code></p>"},{"location":"site-owner/micro-service/#explanation_2","title":"Explanation:","text":"<ul> <li>container_name: name of the container that contains either the baseline or one of the experimental systems</li> <li>datasets/publications: specify if datasets of publications should be recommended</li> <li>itemid: the target item of the recommendations</li> <li>page: the number of the start page</li> <li>rpp: the number of results per page</li> </ul>"},{"location":"site-owner/micro-service/#output_1","title":"Output:","text":"<pre><code>{'itemid': 'M26923455',\n 'itemlist': ['M27852061',\n              'M26673108',\n              'M27894536',\n              'M27293030',\n              'M27133708',\n              'M26841192',\n              'M27144310',\n              'M27353833',\n              'M27287107',\n              'M27658597'],\n 'num_found': 10,\n 'page': 0,\n 'rpp': 10}\n</code></pre>"},{"location":"site-owner/micro-service/#explanation_3","title":"Explanation:","text":"<ul> <li>itemid: the target item of the recommendations</li> <li>itemlist: a list containing the document identifiers</li> <li>num_found: the total number of documents found for the given <code>query</code></li> <li>page: the number of the start page</li> <li>rpp: the number of results per page</li> </ul>"},{"location":"site-owner/server/","title":"STELLA-Server","text":""},{"location":"site-owner/server/#feedback","title":"Feedback","text":"<p>GET details of all feedbacks (see also <code>util/GET_feedbacks.py</code>): <code>/feedbacks</code></p> <p>GET details of feedback with <code>id</code> (see also <code>util/GET_feedback.py</code>): <code>/feedbacks/&lt;int:id&gt;</code></p> <p>POST new feedback for session with <code>id</code> (see also <code>util/POST_feedback.py</code>): <code>/sessions/&lt;int:id&gt;/feedbacks</code></p> <p>The payload should be provided as follows:  </p> <pre><code>{\n    \"start\": \"2019-11-04 00:06:23\",\n    \"end\": \"2019-11-04 00:10:38\",\n    \"interleave\": \"True\",\n    \"clicks\": [\n               {\"1\": {\"doc_id\": \"doc1\", \"clicked\": \"False\", \"date\": \"None\", \"system\": \"EXP\"}},\n               {\"2\": {\"doc_id\": \"doc11\", \"clicked\": \"True\", \"date\": \"2019-11-04 00:08:15\", \"system\": \"BASE\"}},\n               {\"3\": {\"doc_id\": \"doc2\", \"clicked\": \"False\", \"date\": \"None\", \"system\": \"EXP\"}},\n               {\"4\": {\"doc_id\": \"doc12\", \"clicked\": \"True\", \"date\": \"2019-11-04 00:06:23\", \"system\": \"BASE\"}},\n               {\"5\": {\"doc_id\": \"doc3\", \"clicked\": \"False\", \"date\": \"None\", \"system\": \"EXP\"}},\n               {\"6\": {\"doc_id\": \"doc13\", \"clicked\": \"False\", \"date\": \"None\", \"system\": \"BASE\"}},\n               {\"7\": {\"doc_id\": \"doc4\", \"clicked\": \"False\", \"date\": \"None\", \"system\": \"EXP\"}},\n               {\"8\": {\"doc_id\": \"doc14\", \"clicked\": \"False\", \"date\": \"None\", \"system\": \"BASE\"}},\n               {\"9\": {\"doc_id\": \"doc5\", \"clicked\": \"False\", \"date\": \"None\", \"system\": \"EXP\"}},\n               {\"10\": {\"doc_id\": \"doc15\", \"clicked\": \"False\", \"date\": \"None\", \"system\": \"BASE\"}}         \n              ]\n}\n</code></pre> <p>PUT details for feedback with <code>id</code> (see also <code>util/PUT_feedback.py</code>): <code>/feedbacks/&lt;int:id&gt;'</code></p> <p>The payload should be provided as follows:  </p> <pre><code>{\n    \"start\": \"2019-11-04 00:06:23\",\n    \"end\": \"2019-11-04 00:10:38\",\n    \"interleave\": \"True\",\n    \"clicks\": [\n               {\"1\": {\"doc_id\": \"doc1\", \"clicked\": \"False\", \"date\": \"None\", \"system\": \"EXP\"}},\n               {\"2\": {\"doc_id\": \"doc11\", \"clicked\": \"True\", \"date\": \"2019-11-04 00:08:15\", \"system\": \"BASE\"}},\n               {\"3\": {\"doc_id\": \"doc2\", \"clicked\": \"False\", \"date\": \"None\", \"system\": \"EXP\"}},\n               {\"4\": {\"doc_id\": \"doc12\", \"clicked\": \"True\", \"date\": \"2019-11-04 00:06:23\", \"system\": \"BASE\"}},\n               {\"5\": {\"doc_id\": \"doc3\", \"clicked\": \"False\", \"date\": \"None\", \"system\": \"EXP\"}},\n               {\"6\": {\"doc_id\": \"doc13\", \"clicked\": \"False\", \"date\": \"None\", \"system\": \"BASE\"}},\n               {\"7\": {\"doc_id\": \"doc4\", \"clicked\": \"False\", \"date\": \"None\", \"system\": \"EXP\"}},\n               {\"8\": {\"doc_id\": \"doc14\", \"clicked\": \"False\", \"date\": \"None\", \"system\": \"BASE\"}},\n               {\"9\": {\"doc_id\": \"doc5\", \"clicked\": \"False\", \"date\": \"None\", \"system\": \"EXP\"}},\n               {\"10\": {\"doc_id\": \"doc15\", \"clicked\": \"False\", \"date\": \"None\", \"system\": \"BASE\"}}         \n              ]\n}\n</code></pre>"},{"location":"site-owner/server/#participant","title":"Participant","text":"<p>GET all systems of participant with <code>id</code> (see also <code>util/GET_systems_of_participant.py</code>): <code>/participants/&lt;int:id&gt;/systems</code></p> <p>GET all sessions of participant with <code>id</code> (see also <code>util/GET_sessions_of_participant.py</code>): <code>/participants/&lt;int:id&gt;/sessions</code></p>"},{"location":"site-owner/server/#ranking","title":"Ranking","text":"<p>GET details of ranking with <code>id</code> (see also <code>util/GET_ranking.py</code>): <code>/rankings/&lt;int:id&gt;</code></p> <p>GET a list of all ranking <code>id</code>s (see also <code>util/GET_rankings.py</code>): <code>/rankings</code></p> <p>POST ranking for feedback with <code>id</code> (see also <code>util/POST_rankings.py</code>): <code>/feedbacks/&lt;int:id&gt;/rankings</code></p> <p>The payload should be provided as follows:  </p> <pre><code>{\n    \"q\": \"this is the query text\",\n    \"q_date\": \"2019-11-04 00:04:00\",\n    \"q_time\": 325,\n    \"num_found\": 100,\n    \"page\": 1,\n    \"rpp\": 10,\n    \"items\": [\n              {\"1\": \"doc1\", \"2\": \"doc2\", \"3\": \"doc3\", \"4\": \"doc4\", \"5\": \"doc5\", \n               \"6\": \"doc6\", \"7\": \"doc7\", \"8\": \"doc8\", \"9\": \"doc9\", \"10\": \"doc10\"}\n             ]\n}\n</code></pre> <p>PUT ranking with <code>id</code> (see also <code>util/PUT_ranking.py</code>): <code>/rankings/&lt;int:id&gt;</code></p> <p>The payload should be provided as follows:  </p> <pre><code>{\n    \"q\": \"this is the query text\",\n    \"q_date\": \"2019-11-04 00:04:00\",\n    \"q_time\": 325,\n    \"num_found\": 100,\n    \"page\": 1,\n    \"rpp\": 10,\n    \"items\": [\n              {\"1\": \"doc1\", \"2\": \"doc2\", \"3\": \"doc3\", \"4\": \"doc4\", \"5\": \"doc5\", \n               \"6\": \"doc6\", \"7\": \"doc7\", \"8\": \"doc8\", \"9\": \"doc9\", \"10\": \"doc10\"}\n             ]\n}\n</code></pre>"},{"location":"site-owner/server/#recommendation","title":"Recommendation","text":"<p>GET details of recommendation with <code>id</code> (see also <code>util/GET_ranking.py</code> that works analogously): <code>/recommendations/&lt;int:id&gt;</code></p> <p>GET a list of all recommendation <code>id</code>s (see also <code>util/GET_rankings.py</code> that works analogously): <code>/recommendations</code></p> <p>POST recommendation for feedback with <code>id</code> (see also <code>util/POST_rankings.py</code> that works analogously): <code>/feedbacks/&lt;int:id&gt;/recommendations</code></p> <p>The payload should be provided as follows:  </p> <pre><code>{\n    \"q\": \"docid\",\n    \"q_date\": \"2019-11-04 00:04:00\",\n    \"q_time\": 325,\n    \"num_found\": 100,\n    \"page\": 1,\n    \"rpp\": 10,\n    \"items\": [\n              {\"1\": \"doc1\", \"2\": \"doc2\", \"3\": \"doc3\", \"4\": \"doc4\", \"5\": \"doc5\", \n               \"6\": \"doc6\", \"7\": \"doc7\", \"8\": \"doc8\", \"9\": \"doc9\", \"10\": \"doc10\"}\n             ]\n}\n</code></pre> <p>PUT recommendation with <code>id</code> (see also <code>util/PUT_ranking.py</code> that works analogously): <code>/recommendations/&lt;int:id&gt;</code></p> <p>The payload should be provided as follows:  </p> <pre><code>{\n    \"q\": \"docid\",\n    \"q_date\": \"2019-11-04 00:04:00\",\n    \"q_time\": 325,\n    \"num_found\": 100,\n    \"page\": 1,\n    \"rpp\": 10,\n    \"items\": [\n              {\"1\": \"doc1\", \"2\": \"doc2\", \"3\": \"doc3\", \"4\": \"doc4\", \"5\": \"doc5\", \n               \"6\": \"doc6\", \"7\": \"doc7\", \"8\": \"doc8\", \"9\": \"doc9\", \"10\": \"doc10\"}\n             ]\n}\n</code></pre>"},{"location":"site-owner/server/#session","title":"Session","text":"<p>GET session with <code>id</code> (see also <code>util/GET_session.py</code>): <code>/sessions/&lt;int:id&gt;</code></p> <p>GET feedback from session with <code>id</code> (see also <code>util/GET_feedbacks_of_session.py</code>): <code>/sessions/&lt;int:id&gt;/feedbacks</code> </p> <p>GET systems used in session with <code>id</code>: <code>/sessions/&lt;int:id&gt;/systems</code></p>"},{"location":"site-owner/server/#site","title":"Site","text":"<p>GET site details, e.g. <code>id</code>, with the help of the <code>name</code> (see also <code>util/GET_systems_at_site.py</code>): <code>/sites/&lt;string:name&gt;</code></p> <p>GET sessions at site with <code>id</code> (see also <code>util/GET_session_at_site.py</code>): <code>/sites/&lt;int:id&gt;/sessions</code></p> <p>GET systems deployed at site with <code>id</code> (see also <code>util/GET_systems_at_site.py</code>): <code>/sites/&lt;int:id&gt;/systems</code></p> <p>POST new session at site with <code>id</code> (see also <code>util/POST_sessions.py</code>): <code>/sites/&lt;int:id&gt;/sessions</code></p> <p>The payload should be provided as follows:  </p> <pre><code>{\n    \"site_user\": \"123.123.123.123\",\n    \"start\": \"2020-02-20 20:02:20\",\n    \"end\": \"2020-02-20 20:02:20\",\n    \"system_ranking\": \"rank_exp_a\",\n    \"system_recommendation\": \"rec_exp_a\"\n}\n</code></pre>"}]}